---
title: "Last Homework"
author: L. Harvey 
embed-resources: TRUE
format: html
editor: visual
---

## Last Homework!

The final homework assignment for the lovely SOC 722 focuses on Generalized Linear Models and exercises building and interpreting them in R-Studio. Without further adieu:

### Background Code

```{r}
library(tidyverse)
theme_set(theme_light(base_family = "Optima"))
library(conflicted)
conflicts_prefer(dplyr::filter)
library(tidyverse)
library(infer)
library(janitor)
library(gssr)
library(modelsummary)
library(gt)
gss18 <- gss_get_yr(2018)
```

### Exercise 12.1.4: Plotting a Normal Distribution

```{r}
# For f(x) = (x - mu)^2
mu = 0

ggplot() + 
  xlim(-5, 5) + 
  geom_function(fun = \(x) (x - mu)^2) + 
  labs(x = "x", y = "f(x)")

# For f(x) = -(x-mu^2) 
ggplot() + 
  xlim(-5, 5) + 
  geom_function(fun = \(x) -(x - mu)^2) + 
  labs(x = "x", y = "f(x)")

# For f(x) = e^-(x-mu)^2
ggplot() + 
  geom_function(fun = \(x) exp(-(x-mu)^2)) + 
  labs(x = "x", y = "f(x)")
```

### Exercise 12.1.5: Square Rootedness

What is the square root of pi?

```{r}
sqrt(pi)
```

What's the area under the curve of this function?

```{r}
ggplot() + 
  xlim(-5, 5) + 
  geom_function(fun = \(x) 1/sqrt(pi) * exp(-(x)^2)) + labs(x = "x", y = "f(x)")

integrate(f = \(x) 1/sqrt(pi) * exp(-x^2), lower = -Inf, upper = Inf)
```

The area of the curve under the function (excluding absolute error) is 1.

### Exercise 12.1.7: GSS Indicator Practice

Take the 2018 GSS data and use a normal linear regression to estimate `height` from `male`.

```{r}
# Build and recode the variables: 
male_model <- gss18 |> 
  select(height, sex, age) |> 
  haven::zap_missing() |> 
  mutate(male = if_else(sex == 1, 1L, 0L)) |> 
  drop_na()

male_height <- glm(height ~ male, data = male_model, family = gaussian(link = "identity"))

male_height_age <- glm(height ~ male + age, data = male_model, family = gaussian(link = "identity"))

# Viewing the data: 
msummary(list(male_height, male_height_age), output = "gt") |> 
  opt_table_font(font = "Optima")
```

In the first model (male_height), the alpha (i.e. when the value for `male` is 0) is 64.41 (or 5 feet, 4 inches). Also in this first model (male_height), the beta is 5.705. Thus, this beta tells us that being male increases the predicted height of the respondent by 5.705 inches.

In the second model (male_height_age), the alpha (i.e when the values for *both* `male` and `age` are 0) is 64.649. When interpreting this data, note that the value of `age` = 0 can be confusing, as the GSS *doesn't include* children in the data set. Even still, the data can be interpreted as follows: Beta 1 is 5.706, which (just as in model 1), means that being male increases the predicted height of the respondent by 5.706 inches. Beta 2 is -0.001, indicating that as each year of age increases, the predicted respondent height decreases by 0.001 inches,

### Exercise 12.1.8: Polynomial Regression

Using the same 2018 GSS dataset, predict `coninc` (family income) from `age` and `age_squared`.

```{r}
model_two <- gss18 |> 
  select(coninc, age) |> 
  haven::zap_missing() |> 
  mutate(age_squared = age^2) |> 
  drop_na()

income_age_squared <- glm(coninc ~ age + age_squared, data = model_two, family = gaussian(link = "identity")) 

# To view the data: 
msummary(list(income_age_squared), output = "gt") |> 
  opt_table_font(font = "Optima")
```

In this model, the Beta 0 coefficient (AKA the alpha) is -15627.326. Translating this from statistics into English, this means that when `age` is equal to 0, that income is -\$15,627.33 (which is both pretty bad and utter nonsense). Unless all babies start their lives by financing mid-2020s cars, this number is useless for data interpretation purposes.

If we wanted to plot the predictions from this model using a new data set that contains the ages of 18 to 85, it would appear as follows:

```{r}
income_model <- tibble(age = 18:85, age_squared = age^2)

income_model$predicted_income <- predict(income_age_squared, newdata = income_model)

ggplot() + 
  geom_line(aes(x = age, y = predicted_income), 
            color = "pink", 
            data = income_model) 
```

### Exercise 12.1.9: Transformations Centered Around the Mean

Transform the `age` variable so that it's centered around the mean.

```{r}
# Mutate the data to center it: 
centered <- gss18 |> 
  mutate(age_centered = age - mean(age, na.rm = TRUE)) |> 
  mutate(age_squared = age_centered^2) |> 
  select(coninc, age_centered, age_squared) |> 
  drop_na()

# Now let's fit the same model: 
centered_model <- glm(coninc ~ age_centered + age_squared, 
                      data = centered, 
                      family = gaussian(link = "identity"))

msummary(list(income_age_squared, centered_model), output = "gt") |> 
    opt_table_font(font = "Optima")
```

[What changed and what remained the same?\
]{.underline}After fitting the model, the `age_squared` coefficient is unchanged, with a value of -26.706. The AIC *and* BICs for both models are identical, as well.

[What does the Beta 0 coefficient mean here?\
]{.underline}Here, Beta 0 refers to the value at which both `age_centered` and `age_squared` are equal to 0. However, after changing the code to center around the mean, we have successfully removed those babies from debt, with a new intercept centered around the **mean income of the data**: 58,427.559 (as opposed to the original -15,627.326).

### Exercise 12.1.10: Transformations in Standard Deviations

Transforming the `age` variable so that it's in standard deviations away from the mean.

```{r}
# Mutating the variables: 
transformation <- gss18 |> 
  mutate(age_std = (age - mean(age, na.rm = TRUE)) / sd(age, na.rm = TRUE)) |> 
  mutate(age_squared = age_std^2) |> 
  select(coninc, age_std, age_squared) |> 
  drop_na()

stdev_model <- glm(coninc ~ age_std + age_squared, 
                   data = transformation, 
                   family = gaussian(link = "identity"))

# Viewing the model: 
msummary(list(income_age_squared, stdev_model), output = "gt") |> 
  opt_table_font(font = "Optima")
```

[What changed and what remained the same?\
]{.underline}Just as we saw in the earlier model above, the Beta 0 (which represents predicted income when our heavily-modified `age` variable equals 0) remains the same (at -15627.326).

[How would you interpret the Beta 1 and Beta 2 coefficients?\
]{.underline}Unlike models previous, Beta 1 and Beta 2 now have to be understood in terms of their standard deviation distances away from our centralized mean. In other words, we now need to recognize that, these Betas move in tandem with the Alpha (i.e. that the changes in Betas 1 and 2 change consistently with every standard deviation away from the mean).

### Exercise 12.1.11: Comparisons and Regression

For this exercise, we want to build a regression model that predicts `coninc` from `married`.

```{r}
# Start by inputting Andres' code: 
d <- gss18 |> 
  select(marital, coninc, sex) |> 
  mutate(
    coninc = haven::zap_label(coninc),
    sex = haven::as_factor(sex),
    marital = haven::as_factor(marital)
  ) |> 
  drop_na() |> 
  mutate(married = if_else(marital == "married", 1L, 0L)) |> 
  mutate(male = if_else(sex == "male", 1L, 0L))

d |> 
  group_by(married) |> 
  summarize(
    avg_coninc = mean(coninc, na.rm = TRUE), 
    sd = sd(coninc, na.rm = TRUE),
    n = n()
  ) |> 
  mutate(std_error = sd / n())

# Building the model: 
married_model <- glm(coninc ~ married, data = d, 
             family = gaussian(link= "identity"))

msummary(married_model, output = "gt") |> 
  opt_table_font(font = "Optima")
```

When comparing these results to the previous `dplyr` table, the intercept (representing unmarried individuals) equals the `avg_coninc` variable on the `dplyr` table, with both items equaling 36,817.678. The `married` intercept on our fancy new table is 30,674.237---if we add together the Beta 0 (36,817.678) and Beta 1 (30,674.237) from the new regression model, we get the sum of the `married` variable from the original `dplyr` table (67,491.915).

Now we want to calculate the standard error by plugging in the corresponding values in the `dplyr` table.

```{r}
sqrt((34642.61^2/1229) + (45378.47^2/923))
```

[Compare this value to the standard error corresponding to the `married` coefficient. Do they look similar?\
]{.underline}The `married` value from our new table is 1725.002, as compared to our newly-calculated standard error of 1790.945. These numbers are similar, but not the same.

### Exercise 12.1.12: Binary Interactions

```{r}
# Start with Andres' code: 
d |> 
  group_by(male, married) |> 
  summarize(coninc = mean(coninc, na.rm = TRUE))
```

To find the remaining numbers that fill in this table, we want to fit a normal linear regression (as follows):

```{r}
tabley <- glm(coninc ~ male * married, 
              data = d, 
              family = gaussian(link = "identity"))

msummary(tabley, output = "gt") |> 
  opt_table_font(font = "Optima")
```

| male | married | coninc   | regression output                                  |
|------|---------|----------|----------------------------------------------------|
| 0    | 0       | 33560.78 | Beta 0 = 33560.775                                 |
| 0    | 1       | 66759.76 | Beta 0 + Beta 2 = 33560.775 + 33198.981 = 66759.76 |
| 1    | 0       | 41014.66 | Beta 0 + Beta 1 = 41014.66                         |
| 1    | 1       | 68292.14 | Add all Betas = = 68292.14                         |

### Exercise 12.1.13: Time to Ride a Bike

Building a normal regression for "bikeshares"!

```{r}
# Importing the dataset: 
data(bikes, package = "bayesrules")
```

First, create the dataset, centering `windspeed` and `temp_feel` around their mean values:

```{r}
bike_data <- bikes |> 
  mutate(center_windspeed = windspeed - mean(windspeed, na.rm = TRUE)) |> 
  mutate(center_temp = temp_feel - mean(temp_feel, na.rm = TRUE)) |> 
  select(rides, center_windspeed, center_temp, weekend)
```

Now building the model:

```{r}
bike_model <- glm(rides ~ center_windspeed + center_temp + weekend, 
             data = bike_data, 
             family = gaussian(link = "identity"))

#Summary: 
msummary(bike_model, output = "gt") |> 
  opt_table_font(font = "Optima")
```

[If the temperature and the wind speed are average, what is the expected ridership for a weekend day? What is the expected ridership for a weekday? Show the code that gets you your answers.]{.underline}

1.  Beta 0 is a weekday with average windspeed and temperature, which equals 3683.442. Thus, the predicted number of bike rides on this day is 3684 bike rides.
2.  On the weekend, the `weekendTRUE` coefficient gets thrown into the mix (with a value of -713.575). We add our Beta 0 (3683.442) and our `weekendTRUE` coefficient (-713.575) to get a predicted number of 2969.876 (or, rounded, 2970) bike rides.

### Exercise 12.1.14: Riding Bikes with Poisson

The bike ride thing again, but this time, with a Poisson model!

```{r}
fish_on_bikes <- glm (rides ~ center_windspeed + center_temp + weekend,
                      data = bike_data, 
                      family = poisson (link = "log"))

# Model time: 
msummary(fish_on_bikes, output = "gt") |> 
  opt_table_font(font = "Optima")
```

[If the temperature and the wind speed are average, what is the expected ridership for a weekend day? What is the expected ridership for a weekday? Show the code that gets you your answers.]{.underline}

1.  Beta 0 is a weekday with average windspeed and temperature, which equals a log-count of 8.175 riders. Thus, in order to find the predicted number of bike rides on this day, we need to exponentiate this number, which gives us a total of 3551.055 cyclists (or, if we round: 3552 cyclists).
2.  To find the predicted log-count for cyclists on a weekend, we subtract our `weekendTRUE` coefficient (-0.217) from our intercept (8.175) to get a log-count of 7.953. Again, we exponentiate this number, which gives us a total of 2858.351 (or, rounded: 2859) weekend cyclists.

![](images/il_570xN.1148303632_f89p.webp)

### Exercise 12.1.15: Assessing the Goodness of Fit

```{r}
# Andres' code: 
mod_normal <- glm(rides ~ windspeed + temp_feel + weekend, 
                  data = bikes, 
                  family = "gaussian")

bikes$resid <- residuals(mod_normal)

bikes |> 
  ggplot(aes(date, resid)) + 
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed") + 
  geom_point(
    data = filter(bikes, abs(resid) == max(abs(resid))),
    color = "red", shape = 21, size = 3
  )
```

[What can you tell me about the model fit? Why do you think the model fits poorly?\
]{.underline}The fit for this model is poor, as it appears to over-predict in the colder months and under-predict in the warmer ones. Reporting as someone who bikes regularly, it sucks to bike in the colder months (read: January to, like, early April). Consequently, there will likely be less cyclists biking during these time periods. Similarly, biking in the warmer months (say, mid-April through October) is a much more pleasant experience, leading to an increase of cyclists biking during this time. The model does not take into account the seasonality of biking, hence the lack of good fit here.

[What was the date of the observation with the largest residual?]{.underline}\
In order to figure this out, we use the following code:

```{r}
filter(bikes, abs(resid) == max(abs(resid)))
```

This tells us that the big red outlier in this model was from October 29th, 2012---the date that Hurricane Sandy hit the Eastern Seaboard.

This was not a good day to ride a bike, as hurricanes provide a less-than-tranquil cycling experience. Of course, the model could not have accounted for a hurricane to skew its predictions, so we can forgive it for this outlier.

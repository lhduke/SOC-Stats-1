---
title: "Homework #11"
author: L. Harvey 
format: html
embed-resources: TRUE 
editor: visual
---

## Homework #11

Background Code:

```{r}
library(tidyverse)
theme_set(theme_light(base_family = "Optima"))

library(modelsummary)
library(broom)
library(gt)
library(performance)

library(gssr)
gss18 <- gss_get_yr(2018) 

vars <- c(
  "hompop", "sibs", "numwomen", "nummen", "age", "sex", "race", "attend", "polviews", "degree",
  "happy", "sexornt", "premarsx", "condom", "wrkslf", "fefam", "cappun", "padeg", "madeg")

d <- gss18 |> 
  select(all_of(vars)) |> 
  haven::zap_missing() |> 
  ## Continuous variabless 
  mutate(across(all_of(vars[1:5]), haven::zap_labels)) |> 
  ## Categorical variables
  mutate(across(!all_of(vars[1:5]), haven::as_factor)) |>
  mutate(numtot = numwomen + nummen)
```

## Exercise 11.1: Goodness of Fit

### Exercise 11.1.1

Outcome Variable: Is the respondent happy? (Using "happy")\
Predictor Variables: What is respondent's sex ("sex"), what is the highest education respondent has received ("degree"), and how many siblings respondent has ("sibs")?

```{r}
# Mutate the variables: 
first_data <- d |> 
  mutate(
    very_happy = if_else(happy == "very happy", 1L, 0L),  
    male = if_else(sex == "male", 1L, 0L), 
    hs_degree = if_else(degree == "less than high school", 1L, 0L), 
    sibs = as.numeric(sibs)
  ) |> 
  select(very_happy, male, degree, sibs) |> 
  drop_na()

# Running a logistic regression: 
first_log_reg <- glm(very_happy ~ male + degree + sibs, family = "binomial", data = first_data)

# Summarizing the code: 
summary(first_log_reg)

# Checking fit with the Hosmer test: 
performance_hosmer(first_log_reg)
```

Larger p-values are an indication of good fit here because the p-value is about identifying differences (i.e. finding differences that we wouldn't expect to find, based on the model). The higher the p-value is, the better of a fit it is for the model. Thus, this model's p-value of 0.754 tells us that the "model seems to fit well."

(However, large p-values doesn't mean that this model is a great fit---just that there isn't enough evidence of it being a bad-enough fit to warrant rejecting the null hypothesis).

### Exercise 11.1.2

Time to run a link test on the same model from above:

```{r}
first_data$predict_log_odds_very_happy <- predict(first_log_reg)

first_link <- glm(very_happy ~ predict_log_odds_very_happy, 
                  data = first_data, 
                  family = binomial(link = "logit"))

tidy(first_link)
```

Here, the beta value is 1.000+, which indicates that the model is a good fit. Again, keeping in mind that we want the highest possible p-value for the model: As p-values are measured from 0-1, having a p-value of 1.000+ is indicative of a good fit for the model.

## Exercise 11.2: Poisson Regression

### Exercise 11.2.1

Using rpois() to generate 1000 draws from the Poisson distribution for different values of lambda:

```{r}
lamb_vals <- c(1, 2, 3, 4, 5)
lamb_draws <- lapply(lamb_vals, function(lambda) rpois(1000, lambda))

data <- data.frame(value = unlist(lamb_draws),
                   lambda = rep(lamb_vals, each = 1000))

ggplot(data, aes(x = value, fill = as.factor(lambda))) +
  geom_bar(position = "identity", alpha = 0.4, color = "white") +
  facet_wrap(~lambda, scales = "free") +
  coord_cartesian(xlim = c(0, 10), ylim = c(0,100))+
  labs(title = "Poisson Distribution with Different Lambda Values",
       x = "Values",
       y = "Frequency")
```

### Exercise 11.2.2

Create three models that predict "numtot" (the sum of male and female sexual partners) from a subset of predictors. Keep it simple, a la Occam.

[Model Parameters:]{.underline}

1.  Outcome Variable: What is the respondent's total number of male and female sexual partners? (Using "numtot")
2.  Predictor Variables: What is respondent's self-reported happiness level ("happy"), what is respondent's sex ("sex"), what is respondent's age ("age"), and did the respondent use a condom the last time they had sex ("condom")?

```{r}
# Mutate the variables: 
sexy_data <- d |> 
  mutate(
    very_happy = if_else(happy == "very happy", 1L, 0L), 
    male = if_else(sex == "male", 1L, 0L), 
    age = as.numeric(age), 
    nope = if_else(condom == "not used", 1L, 0L) 
  ) |> 
  select(numtot, very_happy, male, age, nope) |> 
  drop_na()

# Making models: 
sexy_model1 <- glm(numtot ~ very_happy + age, 
                   data = sexy_data, 
                   family = "poisson")

sexy_model2 <- glm(numtot ~ very_happy + age + male, 
                   data = sexy_data, 
                   family = "poisson")

sexy_model3 <- glm(numtot ~ very_happy + age + male + nope, 
                   data = sexy_data, 
                   family = "poisson")

# Comparison time! 
msummary(list(sexy_model1, sexy_model2, sexy_model3), output = "gt") |> 
  opt_table_font(font = "Optima")
```

[Interpreting the models:]{.underline}

1.  sexy_model3 fits the data best based on both its AIC and BIC scores, even with the having the largest number of predictor variables included in the calculation.
2.  In sexy_model1, happiness was recoded where 1 = "very happy" and 0 = "not happy." The beta of -0.478 in sexy_model1 means that happy respondents are predicted to have -0.478 less sexual partners than unhappy respondents. (Not sure that logic follows this interpretation.)

### Exercise 11.2.3

Create at least three models that predict "sibs" (the respondent's number of siblings) from a subset of predictors. Again, keep it simple.

[Model Parameters:]{.underline}

1.  Outcome Variable: What is the respondent's number of siblings? (Using "sibs")
2.  Predictor Variables: What is respondent's sex ("sex"), what is respondent's age ("age"), what is respondent's mother's highest completed level of education ("madeg"), and what is respondent's father's highest completed level of education ("padeg")?

```{r}
# Mutate the variables: 
final_data <- d |> 
  mutate(
    male = if_else(sex == "male", 1L, 0L), 
    age = as.numeric(age), 
    mom_hs_degree = if_else(madeg == "high school", 1L, 0L), 
    pop_hs_degree = if_else(padeg == "high school", 1L, 0L) 
  ) |> 
  select(sibs, male, age, mom_hs_degree, pop_hs_degree) |> 
  drop_na()

# Making models: 
last_model1 <- glm(sibs ~ male + age, 
                   data = final_data, 
                   family = "poisson")

last_model2 <- glm(sibs ~ male + age + mom_hs_degree, 
                   data = final_data, 
                   family = "poisson")

last_model3 <- glm(sibs ~ male + age + mom_hs_degree + pop_hs_degree, 
                   data = final_data, 
                   family = "poisson")

# Comparison time! 
msummary(list(last_model1, last_model2, last_model3), output = "gt") |> 
  opt_table_font(font = "Optima")
```

[Interpreting the models:]{.underline}

1.  last_model3 fits the data best based on its AIC score, but last_model2 has the lowest BIC score of the three models.
2.  In last_model1, "male" was recoded to where 1 = male and 0 = female. The beta of -0.091 in last_model1 means that male respondents are predicted to have -0.091 less siblings than female respondents.
